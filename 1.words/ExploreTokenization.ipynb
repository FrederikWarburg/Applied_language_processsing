{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook outlines several methods for tokenizing text into words (and sentences), including:\n",
    "\n",
    "* whitespace\n",
    "* nltk (Penn Treebank tokenizer)\n",
    "* nltk (Twitter-aware)\n",
    "* spaCy\n",
    "* custom regular expressions\n",
    "\n",
    "highlighting differences between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, json\n",
    "import spacy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy lemmatization needs tagger but disable the rest\n",
    "nlp = spacy.load('en', disable=['tagger,ner,parser'])\n",
    "nlp.remove_pipe('tagger')\n",
    "nlp.remove_pipe('ner')\n",
    "nlp.remove_pipe('parser');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tweets_from_json(filename):\n",
    "    tweets=[]\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        data=json.load(file)\n",
    "        for tweet in data:\n",
    "            tweets.append(tweet[\"text\"])\n",
    "    return tweets        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trump_tweets.json comes from the Trump Twitter collection here (downloaded 1/19/19)\n",
    "http://www.trumptwitterarchive.com/archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"../data/trump_tweets.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/frederikwarburg/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=read_tweets_from_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitespace_tokens=[]\n",
    "for tweet in tweets:\n",
    "    whitespace_tokens.append(tweet.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Be sure to tune in and watch Donald Trump on Late Night with David Letterman as he presents the Top Ten List tonight!'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tokens=[]\n",
    "for tweet in tweets:\n",
    "    nltk_tokens.append(nltk.word_tokenize(tweet, language=\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_casual_tokens=[]\n",
    "for tweet in tweets:\n",
    "    nltk_casual_tokens.append(nltk.casual_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_tokens=[]\n",
    "for tweet in tweets:\n",
    "    spacy_tokens.append([token.text for token in nlp(tweet)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorter version of http://sentiment.christopherpotts.net/code-data/happyfuntokenizing.py\n",
    "\n",
    "# The order here is important (match from first to last)\n",
    "\n",
    "# Keep usernames together (any token starting with @, followed by A-Z, a-z, 0-9)\n",
    "regexes=(r\"(?:@[\\w_]+)\",\n",
    "\n",
    "# Keep hashtags together (any token starting with #, followed by A-Z, a-z, 0-9, _, or -)\n",
    "r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",\n",
    "\n",
    "# Keep words with apostrophes, hyphens and underscores together\n",
    "r\"(?:[a-z][a-z’'\\-_]+[a-z])\",\n",
    "\n",
    "# Keep all other sequences of A-Z, a-z, 0-9, _ together\n",
    "r\"(?:[\\w_]+)\",\n",
    "\n",
    "# Everything else that's not whitespace\n",
    "r\"(?:\\S)\"\n",
    ")\n",
    "\n",
    "big_regex=\"|\".join(regexes)\n",
    "\n",
    "my_extensible_tokenizer = re.compile(big_regex, re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "def my_extensible_tokenize(text):\n",
    "    return my_extensible_tokenizer.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "extensible_tokens=[]\n",
    "for tweet in tweets:\n",
    "    extensible_tokens.append(my_extensible_tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(?:\\\\#+[\\\\w_]+[\\\\w\\\\'_\\\\-]*[\\\\w_]+)\""
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Write a function to print out the first 5 tokenized tweets in each of the five tokenizers above. Examine those tweets; how would you characterize the differences?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet  0\n",
      "regex  0 []\n",
      "regex  1 []\n",
      "regex  2 ['exico', 'doing', 'stop', 'the', 'aravan', 'which', 'now', 'fully', 'formed', 'and', 'heading', 'the', 'nited', 'tates', 'stopped', 'the', 'last', 'two', 'many', 'are', 'still', 'exico', 'but', 'can’t', 'get', 'through', 'our', 'all', 'but', 'takes', 'lot', 'order', 'gents', 'there', 'all', 'easy']\n",
      "regex  3 ['Mexico', 'is', 'doing', 'NOTHING', 'to', 'stop', 'the', 'Caravan', 'which', 'is', 'now', 'fully', 'formed', 'and', 'heading', 'to', 'the', 'United', 'States', 'We', 'stopped', 'the', 'last', 'two', 'many', 'are', 'still', 'in', 'Mexico', 'but', 'can', 't', 'get', 'through', 'our', 'Wall', 'but', 'it', 'takes', 'a', 'lot', 'of', 'Border', 'Agents', 'if', 'there', 'is', 'no', 'Wall', 'Not', 'easy']\n",
      "regex  4 ['M', 'e', 'x', 'i', 'c', 'o', 'i', 's', 'd', 'o', 'i', 'n', 'g', 'N', 'O', 'T', 'H', 'I', 'N', 'G', 't', 'o', 's', 't', 'o', 'p', 't', 'h', 'e', 'C', 'a', 'r', 'a', 'v', 'a', 'n', 'w', 'h', 'i', 'c', 'h', 'i', 's', 'n', 'o', 'w', 'f', 'u', 'l', 'l', 'y', 'f', 'o', 'r', 'm', 'e', 'd', 'a', 'n', 'd', 'h', 'e', 'a', 'd', 'i', 'n', 'g', 't', 'o', 't', 'h', 'e', 'U', 'n', 'i', 't', 'e', 'd', 'S', 't', 'a', 't', 'e', 's', '.', 'W', 'e', 's', 't', 'o', 'p', 'p', 'e', 'd', 't', 'h', 'e', 'l', 'a', 's', 't', 't', 'w', 'o', '-', 'm', 'a', 'n', 'y', 'a', 'r', 'e', 's', 't', 'i', 'l', 'l', 'i', 'n', 'M', 'e', 'x', 'i', 'c', 'o', 'b', 'u', 't', 'c', 'a', 'n', '’', 't', 'g', 'e', 't', 't', 'h', 'r', 'o', 'u', 'g', 'h', 'o', 'u', 'r', 'W', 'a', 'l', 'l', ',', 'b', 'u', 't', 'i', 't', 't', 'a', 'k', 'e', 's', 'a', 'l', 'o', 't', 'o', 'f', 'B', 'o', 'r', 'd', 'e', 'r', 'A', 'g', 'e', 'n', 't', 's', 'i', 'f', 't', 'h', 'e', 'r', 'e', 'i', 's', 'n', 'o', 'W', 'a', 'l', 'l', '.', 'N', 'o', 't', 'e', 'a', 's', 'y', '!']\n",
      "\n",
      "Tweet  1\n",
      "regex  0 []\n",
      "regex  1 []\n",
      "regex  2 ['any', 'people', 'are', 'saying', 'that', 'the', 'ainstream', 'edia', 'will', 'have', 'very', 'hard', 'time', 'restoring', 'credibility', 'because', 'the', 'way', 'they', 'have', 'treated', 'over', 'the', 'past', 'years', 'including', 'the', 'election', 'lead-up', 'highlighted', 'the', 'disgraceful', 'uzzfeed', 'story', 'amp', 'the', 'even', 'more', 'disgraceful', 'coverage']\n",
      "regex  3 ['Many', 'people', 'are', 'saying', 'that', 'the', 'Mainstream', 'Media', 'will', 'have', 'a', 'very', 'hard', 'time', 'restoring', 'credibility', 'because', 'of', 'the', 'way', 'they', 'have', 'treated', 'me', 'over', 'the', 'past', '3', 'years', 'including', 'the', 'election', 'lead', 'up', 'as', 'highlighted', 'by', 'the', 'disgraceful', 'Buzzfeed', 'story', 'amp', 'the', 'even', 'more', 'disgraceful', 'coverage']\n",
      "regex  4 ['M', 'a', 'n', 'y', 'p', 'e', 'o', 'p', 'l', 'e', 'a', 'r', 'e', 's', 'a', 'y', 'i', 'n', 'g', 't', 'h', 'a', 't', 't', 'h', 'e', 'M', 'a', 'i', 'n', 's', 't', 'r', 'e', 'a', 'm', 'M', 'e', 'd', 'i', 'a', 'w', 'i', 'l', 'l', 'h', 'a', 'v', 'e', 'a', 'v', 'e', 'r', 'y', 'h', 'a', 'r', 'd', 't', 'i', 'm', 'e', 'r', 'e', 's', 't', 'o', 'r', 'i', 'n', 'g', 'c', 'r', 'e', 'd', 'i', 'b', 'i', 'l', 'i', 't', 'y', 'b', 'e', 'c', 'a', 'u', 's', 'e', 'o', 'f', 't', 'h', 'e', 'w', 'a', 'y', 't', 'h', 'e', 'y', 'h', 'a', 'v', 'e', 't', 'r', 'e', 'a', 't', 'e', 'd', 'm', 'e', 'o', 'v', 'e', 'r', 't', 'h', 'e', 'p', 'a', 's', 't', '3', 'y', 'e', 'a', 'r', 's', '(', 'i', 'n', 'c', 'l', 'u', 'd', 'i', 'n', 'g', 't', 'h', 'e', 'e', 'l', 'e', 'c', 't', 'i', 'o', 'n', 'l', 'e', 'a', 'd', '-', 'u', 'p', ')', ',', 'a', 's', 'h', 'i', 'g', 'h', 'l', 'i', 'g', 'h', 't', 'e', 'd', 'b', 'y', 't', 'h', 'e', 'd', 'i', 's', 'g', 'r', 'a', 'c', 'e', 'f', 'u', 'l', 'B', 'u', 'z', 'z', 'f', 'e', 'e', 'd', 's', 't', 'o', 'r', 'y', '&', 'a', 'm', 'p', ';', 't', 'h', 'e', 'e', 'v', 'e', 'n', 'm', 'o', 'r', 'e', 'd', 'i', 's', 'g', 'r', 'a', 'c', 'e', 'f', 'u', 'l', 'c', 'o', 'v', 'e', 'r', 'a', 'g', 'e', '!']\n",
      "\n",
      "Tweet  2\n",
      "regex  0 []\n",
      "regex  1 []\n",
      "regex  2 ['conomy', 'one', 'the', 'best', 'our', 'history', 'with', 'unemployment', 'year', 'low', 'and', 'the', 'tock', 'arket', 'ready', 'again', 'break', 'record', 'set', 'many', 'times', 'amp', 'all', 'you', 'heard', 'yesterday', 'based', 'phony', 'story', 'was', 'mpeachment', 'want', 'see', 'tock', 'arket', 'rash', 'mpeach', 'rump']\n",
      "regex  3 ['The', 'Economy', 'is', 'one', 'of', 'the', 'best', 'in', 'our', 'history', 'with', 'unemployment', 'at', 'a', '50', 'year', 'low', 'and', 'the', 'Stock', 'Market', 'ready', 'to', 'again', 'break', 'a', 'record', 'set', 'by', 'us', 'many', 'times', 'amp', 'all', 'you', 'heard', 'yesterday', 'based', 'on', 'a', 'phony', 'story', 'was', 'Impeachment', 'You', 'want', 'to', 'see', 'a', 'Stock', 'Market', 'Crash', 'Impeach', 'Trump']\n",
      "regex  4 ['T', 'h', 'e', 'E', 'c', 'o', 'n', 'o', 'm', 'y', 'i', 's', 'o', 'n', 'e', 'o', 'f', 't', 'h', 'e', 'b', 'e', 's', 't', 'i', 'n', 'o', 'u', 'r', 'h', 'i', 's', 't', 'o', 'r', 'y', ',', 'w', 'i', 't', 'h', 'u', 'n', 'e', 'm', 'p', 'l', 'o', 'y', 'm', 'e', 'n', 't', 'a', 't', 'a', '5', '0', 'y', 'e', 'a', 'r', 'l', 'o', 'w', ',', 'a', 'n', 'd', 't', 'h', 'e', 'S', 't', 'o', 'c', 'k', 'M', 'a', 'r', 'k', 'e', 't', 'r', 'e', 'a', 'd', 'y', 't', 'o', 'a', 'g', 'a', 'i', 'n', 'b', 'r', 'e', 'a', 'k', 'a', 'r', 'e', 'c', 'o', 'r', 'd', '(', 's', 'e', 't', 'b', 'y', 'u', 's', 'm', 'a', 'n', 'y', 't', 'i', 'm', 'e', 's', ')', '-', '&', 'a', 'm', 'p', ';', 'a', 'l', 'l', 'y', 'o', 'u', 'h', 'e', 'a', 'r', 'd', 'y', 'e', 's', 't', 'e', 'r', 'd', 'a', 'y', ',', 'b', 'a', 's', 'e', 'd', 'o', 'n', 'a', 'p', 'h', 'o', 'n', 'y', 's', 't', 'o', 'r', 'y', ',', 'w', 'a', 's', 'I', 'm', 'p', 'e', 'a', 'c', 'h', 'm', 'e', 'n', 't', '.', 'Y', 'o', 'u', 'w', 'a', 'n', 't', 't', 'o', 's', 'e', 'e', 'a', 'S', 't', 'o', 'c', 'k', 'M', 'a', 'r', 'k', 'e', 't', 'C', 'r', 'a', 's', 'h', ',', 'I', 'm', 'p', 'e', 'a', 'c', 'h', 'T', 'r', 'u', 'm', 'p', '!']\n",
      "\n",
      "Tweet  3\n",
      "regex  0 ['@newtgingrich']\n",
      "regex  1 []\n",
      "regex  2 ['newtgingrich', 'just', 'stated', 'that', 'there', 'has', 'been', 'president', 'since', 'braham', 'incoln', 'who', 'has', 'been', 'treated', 'worse', 'more', 'unfairly', 'the', 'media', 'than', 'your', 'favorite', 'resident', 'the', 'same', 'time', 'there', 'has', 'been', 'president', 'who', 'has', 'accomplished', 'more', 'his', 'first', 'two', 'years', 'office']\n",
      "regex  3 ['newtgingrich', 'just', 'stated', 'that', 'there', 'has', 'been', 'no', 'president', 'since', 'Abraham', 'Lincoln', 'who', 'has', 'been', 'treated', 'worse', 'or', 'more', 'unfairly', 'by', 'the', 'media', 'than', 'your', 'favorite', 'President', 'me', 'At', 'the', 'same', 'time', 'there', 'has', 'been', 'no', 'president', 'who', 'has', 'accomplished', 'more', 'in', 'his', 'first', 'two', 'years', 'in', 'office']\n",
      "regex  4 ['.', '@', 'n', 'e', 'w', 't', 'g', 'i', 'n', 'g', 'r', 'i', 'c', 'h', 'j', 'u', 's', 't', 's', 't', 'a', 't', 'e', 'd', 't', 'h', 'a', 't', 't', 'h', 'e', 'r', 'e', 'h', 'a', 's', 'b', 'e', 'e', 'n', 'n', 'o', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', 's', 'i', 'n', 'c', 'e', 'A', 'b', 'r', 'a', 'h', 'a', 'm', 'L', 'i', 'n', 'c', 'o', 'l', 'n', 'w', 'h', 'o', 'h', 'a', 's', 'b', 'e', 'e', 'n', 't', 'r', 'e', 'a', 't', 'e', 'd', 'w', 'o', 'r', 's', 'e', 'o', 'r', 'm', 'o', 'r', 'e', 'u', 'n', 'f', 'a', 'i', 'r', 'l', 'y', 'b', 'y', 't', 'h', 'e', 'm', 'e', 'd', 'i', 'a', 't', 'h', 'a', 'n', 'y', 'o', 'u', 'r', 'f', 'a', 'v', 'o', 'r', 'i', 't', 'e', 'P', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', ',', 'm', 'e', '!', 'A', 't', 't', 'h', 'e', 's', 'a', 'm', 'e', 't', 'i', 'm', 'e', 't', 'h', 'e', 'r', 'e', 'h', 'a', 's', 'b', 'e', 'e', 'n', 'n', 'o', 'p', 'r', 'e', 's', 'i', 'd', 'e', 'n', 't', 'w', 'h', 'o', 'h', 'a', 's', 'a', 'c', 'c', 'o', 'm', 'p', 'l', 'i', 's', 'h', 'e', 'd', 'm', 'o', 'r', 'e', 'i', 'n', 'h', 'i', 's', 'f', 'i', 'r', 's', 't', 't', 'w', 'o', 'y', 'e', 'a', 'r', 's', 'i', 'n', 'o', 'f', 'f', 'i', 'c', 'e', '!']\n",
      "\n",
      "Tweet  4\n",
      "regex  0 []\n",
      "regex  1 []\n",
      "regex  2 ['ill', 'leaving', 'for', 'over', 'with', 'the', 'families', 'very', 'special', 'people', 'who', 'lost', 'their', 'lives', 'service', 'our', 'ountry']\n",
      "regex  3 ['Will', 'be', 'leaving', 'for', 'Dover', 'to', 'be', 'with', 'the', 'families', 'of', '4', 'very', 'special', 'people', 'who', 'lost', 'their', 'lives', 'in', 'service', 'to', 'our', 'Country']\n",
      "regex  4 ['W', 'i', 'l', 'l', 'b', 'e', 'l', 'e', 'a', 'v', 'i', 'n', 'g', 'f', 'o', 'r', 'D', 'o', 'v', 'e', 'r', 't', 'o', 'b', 'e', 'w', 'i', 't', 'h', 't', 'h', 'e', 'f', 'a', 'm', 'i', 'l', 'i', 'e', 's', 'o', 'f', '4', 'v', 'e', 'r', 'y', 's', 'p', 'e', 'c', 'i', 'a', 'l', 'p', 'e', 'o', 'p', 'l', 'e', 'w', 'h', 'o', 'l', 'o', 's', 't', 't', 'h', 'e', 'i', 'r', 'l', 'i', 'v', 'e', 's', 'i', 'n', 's', 'e', 'r', 'v', 'i', 'c', 'e', 't', 'o', 'o', 'u', 'r', 'C', 'o', 'u', 'n', 't', 'r', 'y', '!']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def write_tweets(tweets, regexes, max_):\n",
    "    for x, tweet in enumerate(tweets):\n",
    "        print(\"Tweet \", x)\n",
    "        for i in range(len(regexes)):\n",
    "            tmp = re.compile(regexes[i])\n",
    "            print(\"regex \", i, tmp.findall(tweet))\n",
    "        print()\n",
    "\n",
    "        if x >= max_ - 1:\n",
    "            break\n",
    "            \n",
    "write_tweets(tweets, regexes, 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweets have both grammatical and contentual differences. Some a addressing the economy, others border control. They are all consusive. Some are very aggresive, others more formel. This is deduced from the length of the sentences and the grammatical signs e.g. ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Write a function `compare(tokenization_one, tokenization_two)` that compares two tokenizations of the same text and finds the 20 most frequent tokens that don't appear in the other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(tokenization_one, tokenization_two, max_ = 20):\n",
    "\n",
    "    tokenization_one = [item for sublist in tokenization_one for item in sublist]\n",
    "    tokenization_one_dict = dict((x,tokenization_one.count(x)) for x in set(tokenization_one))\n",
    "    tokenization_one_dict = sorted(tokenization_one_dict.items(), key=lambda x: x[1],reverse=True)\n",
    "\n",
    "    tokenization_two = [item for sublist in tokenization_two for item in sublist]\n",
    "    count = 0\n",
    "    result = {}\n",
    "    for key, value in tokenization_one_dict:\n",
    "        if key not in tokenization_two:\n",
    "            result[key] = value\n",
    "            count += 1\n",
    "        if count >= 19:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\"': 24807,\n",
       " '@realDonaldTrump': 8661,\n",
       " '#Trump2016': 840,\n",
       " '@BarackObama': 732,\n",
       " \"don't\": 626,\n",
       " '#MakeAmericaGreatAgain': 560,\n",
       " '@FoxNews': 547,\n",
       " \"I'm\": 524,\n",
       " '@foxandfriends': 504,\n",
       " \"can't\": 423,\n",
       " '@ApprenticeNBC': 393,\n",
       " '@MittRomney': 314,\n",
       " \"It's\": 304,\n",
       " \"it's\": 303,\n",
       " '🇺': 300,\n",
       " '🇸': 300,\n",
       " '#CelebApprentice': 289,\n",
       " '@CNN': 285,\n",
       " \"you're\": 276}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = compare(nltk_casual_tokens, nltk_tokens)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Use one of the NLTK tokenizers; write code to determine how many sentences are in this dataset, and what the average number of words per sentence is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of senteces:  70491\n",
      "Number of words:  885028\n",
      "Number of unique words:  56681\n"
     ]
    }
   ],
   "source": [
    "nltk_tokens_senteces=[]\n",
    "for tweet in tweets:\n",
    "    nltk_tokens_senteces.append(nltk.sent_tokenize(tweet, language=\"english\"))\n",
    "        \n",
    "import numpy as np\n",
    "nltk_tokens_sentence_flat = [item for sublist in nltk_tokens_senteces for item in sublist]\n",
    "print(\"Number of senteces: \", len(nltk_tokens_sentence_flat))\n",
    "nltk_tokens_flat = [item for sublist in nltk_tokens for item in sublist]\n",
    "print(\"Number of words: \", len(nltk_tokens_flat))\n",
    "nltk_tokens_unique = np.unique(nltk_tokens_flat)\n",
    "print(\"Number of unique words: \", len(nltk_tokens_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence length:  60.863982636081204\n"
     ]
    }
   ],
   "source": [
    "sentence_len = [len(sentence) for sentence in nltk_tokens_sentence_flat]\n",
    "avg_len = np.mean(sentence_len)\n",
    "print(\"Average sentence length: \", avg_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4 (check-plus): modify the extensible tokenizer above to keep urls together (e.g., www.google.com or http://www.google.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep usernames together (any token starting with @, followed by A-Z, a-z, 0-9)\n",
    "regexes=(r\"(?:@[\\w_]+)\",\n",
    "\n",
    "# Keep hashtags together (any token starting with #, followed by A-Z, a-z, 0-9, _, or -)\n",
    "r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",\n",
    "\n",
    "# Keep urls together\n",
    "r\"(?:https?:\\/\\/.+[\\w_]+)\",\n",
    "         \n",
    "# Keep words with apostrophes, hyphens and underscores together\n",
    "r\"(?:[a-z][a-z’'\\-_]+[a-z])\",\n",
    "\n",
    "# Keep all other sequences of A-Z, a-z, 0-9, _ together\n",
    "r\"(?:[\\w_]+)\",\n",
    "\n",
    "# Everything else that's not whitespace\n",
    "r\"(?:\\S)\"\n",
    ")\n",
    "\n",
    "big_regex=\"|\".join(regexes)\n",
    "\n",
    "my_url_extensible_tokenizer = re.compile(big_regex, re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "def my_extensible_tokenize_with_urls(text):\n",
    "    return my_url_extensible_tokenizer.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "course\n",
      "website\n",
      "is\n",
      "http://people.ischool.berkeley.edu/~dbamman/info256.html\n"
     ]
    }
   ],
   "source": [
    "print ('\\n'.join(my_extensible_tokenize_with_urls(\"The course website is http://people.ischool.berkeley.edu/~dbamman/info256.html\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r\"(?:@[\\w_]+)|(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)|(?:https?:\\/\\/.+[\\w_]+)|(?:[a-z][a-z’'\\-_]+[a-z])|(?:[\\w_]+)|(?:\\S)\",\n",
       "re.IGNORECASE|re.UNICODE|re.VERBOSE)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_url_extensible_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python anpl",
   "language": "python",
   "name": "anpl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
